#!/usr/bin/env python
# coding: utf-8
'''
Author: Kevin Luke 
CluCHIME.py was made for MSc thesis project at TIFR, Mumbai
Date created:
Date last modified: Jan 20 2022
'''


from iautils.conversion import chime_intensity as ci
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
import numpy as np

from sklearn.neighbors import LocalOutlierFactor
from sklearn.datasets import make_blobs
from numpy import quantile, where, random
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
from sklearn.cluster import DBSCAN
import time
from numpy import asarray
from numpy import savetxt 
from sklearn.cluster import OPTICS, cluster_optics_dbscan
import hdbscan
import seaborn as sns
from numpy import save
from numpy import savez_compressed 

import datetime, time
import numpy as np
import matplotlib.pyplot as plt
from frb_common import common_utils
from iautils import cascade, spectra, spectra_utils
from iautils.conversion import chime_intensity
from numpy import inf
import glob 


class CluCHIME:
    ''' 
    This is a Python library for handling radio astronomical 
    data from 3 beams/detectors from CHIME/FRB instrument and 
    performing CLUSTERING ANALYSIS and RFI removal on the multi 
    beam data .The data format used is .msgpack which is the format 
    used for the INTENSITY data generated by the CHIME/FRB pipeline. 
    However with minimal modifications to inputs the library can also 
    be used for multi beam data from any radio interferometric telescope.
    The CluCHIME library and the tools here were made during my MSc thesis 
    at TATA INSTITUTE OF FUNDAMENTAL RESEARCH under guidance of Dr. Shriharsh 
    Tendulkar, Reader, DEPARTMENT OF ASTRONOMY AND ASTROPHYSICS. Nikhil Londhe 
    of IIT GANDHINAGAR helped me understand array manipulation in NUMPY.
    The library is in primitive phase now and in future will be updated 
    accordinlgy by me. The results obtained from the CLUSTERING ANALYSIS 
    are mixed. This CluCHIME library also streamlines and eases the various
    relevant data  analysis from IAUTILS Python library. 
    The IAUTILS library was developed by Dr. Shriharsh Tendulkar et al 
    at CHIME/FRB Collaboration. For running CluCHIME library successfully you will
    need NUMPY, SCIPY, Scikit-Learn, IAUTILS and frb-common utils preinstalled.
    The later 2 libraries are closed CHIME/FRB libraries from GITHUB. The code 
    is slow and not efficiet in terms of memory usage and needs to updated 
    accordingly in future. The average run times for CLUSTERING ANALYSIS on 
    full data is ~6 hrs on  Dr. Shriharsh Tendulkar's workstation PRITHVI 
    at TIFR which has 128 GB ram, 14 TB of disk space. On regular PC's
    the run times will be much higher.
    '''
    
    def __init__(self,function):
        ''' Initialise the CluCHIME class'''
        
        self.function=function #input any string here 
        
        
        
    def Prepdata(self,
                 pathname1 , 
                 pathname2,
                 pathname3):
        ''' 
        The input are strings of path of the  beam data on disk.
        In Prepdata function the data is collected from the folder where 
        it is stored in .msgpack format. The data in .msgpack is in form of
        NUMPY arrays. The metadata from .msgpack is also saved which will 
        be used for later analysis. Once data is collected it is collated into 
        a mega NUMPY array then the mega array is normalised using individual 
        beam to retain information from all 3 beams during normalisation. The
        normalisation is then saved which again will be used for later analysis.
        Later the normalisation,unnormalised mega array and  the normalised mega 
        array are saved. The individual .msg datapackets has shape 16384,1024 
        which corresponds to 16384 frequency channels between 400 to 800 MHZ and 1024 
        timestamps which corresponds to roughly 1 second of observation.For each 
        beam there will be series of such .msgpack data packets leading to 
        shape of 16384, n*1024 where n corresponds to no of individual data pakcets
        and seconds of observation. If there are 10 seconds of data then the n=10  
        packets are there etc. The mega array whether normalised or unnormalised has final 
        shape of 16384,n*1024,3 where 3 corresponds to 3 beams for which this CluCHIME 
        library was developed.
        '''
        
        self.pathname1=pathname1
        self.pathname2=pathname2
        self.pathname3=pathname3
        
        '''Collecting the data from designated path'''
        filelist1=glob.glob(self.pathname1)
        filelist1.sort()
        I1,Weight1,bins1,fpga01st,fpgan1st,frame_nano1,rfi_mask1=chime_intensity.unpack_datafiles(filelist1)


        filelist2=glob.glob(self.pathname2)
        filelist2.sort()
        I2,Weight2,bins2,fpga02nd,fpgan2nd,frame_nano2,rfi_mask2=chime_intensity.unpack_datafiles(filelist2)


        filelist3=glob.glob(self.pathname3)
        filelist3.sort()
        I3,Weight3,bins3,fpga03rd,fpgan3rd,frame_nano3,rfi_mask3=chime_intensity.unpack_datafiles(filelist3)
         
        '''Savingthe metadata from .msgpack data'''    
        #metadata
        Weight=np.array([Weight1,Weight2,Weight3])
        savez_compressed('Weight.npz',Weight)

        fpgan=np.array([fpgan1st,fpgan2nd,fpgan3rd])
        savez_compressed('fpgan.npz',fpgan)

        fpga0=np.array([fpga01st,fpga02nd,fpga03rd])
        savez_compressed('fpga0.npz',fpga0)

        rfi_mask=np.array([rfi_mask1,rfi_mask2,rfi_mask3,])
        savez_compressed('rfi_mask.npz',rfi_mask)

        frame_nano=np.array([frame_nano1,frame_nano2,frame_nano3])
        savez_compressed('frame_nano.npz',frame_nano)

        bins=np.array([bins1,bins2,bins3])
        savez_compressed('bins.npz',bins)
        
        '''processing and collating the data'''
        #collating the 3 beam data into one mega array
        INT_un=np.array([I1,I2,I3])  #unclean unnormalised mega array
        #declaring the normalisation
        INT_combined1=1/((I1**2+I2**2+I3**2)**(1/2))
        #removing any large values from the normalisation
        INT_combined1[INT_combined1== inf] = 1 #normalisaion array
        #normalising the mega array with the normalisation
        INT_n0=INT_un*INT_combined1
        INT_n=INT_n0.T  #unclean normalised mega array
        #saving the 3 arrays from above process in .npz format on disk
        savez_compressed('INT_combined1.npz',INT_combined1)
        savez_compressed('INT_un.npz',INT_un)
        savez_compressed('INT_n.npz',INT_n)
        
        
        
        
        
    def Sub(self,
            INT_un,
            INT_n,
            INT_combined1):
        '''
        Here we use Cluster exemplar subtraction method.
        We feed the unnormalised intensity array at INT_un and
        the normalised intensity array INT_n and the normalisation array INT_combined.
        Here we perform HDBSCAN clustering on the normalised data from Prepdata.
        Labels and Probaility are also  stored from  and they are stored. Exemplars 
        are also stored from HDBSCAN. Then the following algorithm is used:
        1) Do clustering and save the exemplars.
        2) Subtract the nearest exemplar from the normalized 3 beam data.
        3) Multiply the normalization with subtracted data.
        More details is in Thesis.
        '''
        
        self.INT_un=INT_un
        self.INT_n=INT_n
        self.INT_combined1=INT_combined1
       
        INT_un=np.load('INT_un.npz')
        INT_un=INT_un['arr_0']
        
        INT_n=np.load('INT_n.npz')
        INT_n=INT_n['arr_0']
        
        INT_combined1=np.load('INT_combined1.npz')
        INT_combined1=INT_combined1['arr_0']
       
        
    
        
        #entire 16384 channels HDBSCAN
        start=time.time()

        INT_prob1=[]  #probability
        INT_2d=[]     #labels
        INT_exemp=[]  #exemplars
        for i in range(0,INT_n.shape[1]):
            clusterer=hdbscan.HDBSCAN(min_cluster_size=20,
                                      min_samples=None)
            clusterer.fit(INT_n[:,i,:])
            INT_pred=clusterer.labels_
            n_clusters_=len(set(INT_pred)) -(1 if -1 in INT_pred else 0)
            INT_pred=clusterer.labels_
            INT_prob=clusterer.probabilities_

            INT_exemp.append(clusterer.exemplars_)
            INT_2d.append(INT_pred)
            INT_prob1.append(INT_prob)


        INT_prob1=np.array(INT_prob1)
        INT_2d=np.array(INT_2d)
        INT_exemp=np.array(INT_exemp) 



        savez_compressed('INT_prob1.npz',INT_prob1)
        savez_compressed('INT_2d.npz',INT_2d)
        savez_compressed('INT_exemp.npz',INT_exemp)



        #exemplar subtraction


        INT_n_part=INT_n
        INT_2d_part=INT_2d
        INT_exemp_part=INT_exemp
        INTnew=np.zeros_like(INT_n_part)     #the cleaned norm array
        ARGMIN=[]

        for i in range(0,INT_2d.shape[0]):
            for j in range(0,INT_2d.shape[1]):
                INT_exemp_combined=np.vstack(INT_exemp_part[i])

                label=INT_2d_part[i,j]
                value=INT_n_part[j,i,:]

                if label!=-1:

                    argmin=np.argmin(np.sum((value-INT_exemp_part[i][label])**2,axis=1))
                    INTnew[j,i,:]=value-(INT_exemp_part[i][label][argmin])

                else:
                    argmin=np.argmin(np.sum((value-INT_exemp_combined)**2,axis=1))
                    INTnew[j,i,:]=value-INT_exemp_combined[argmin]

                ARGMIN.append(argmin)    


        end=time.time()
        print(end-start)
        print(INTnew.shape)    
        savez_compressed('INTnew.npz',INTnew)
        
        
        #denormalise the array
        INTnewnorm_whole=(INTnew.T)/INT_combined1 #cleaned unnorm array
        savez_compressed('INTnewnorm_whole.npz',(INTnewnorm_whole))
 

    def Single(self,
               INT_un,
               INT_n,
               i):
        '''
        We perform HDBSCAN clustering on single channels of frequency.
        We feed the unnormalised intensity array at INT_un and
        the normalised intensity array INT_n and the normalisation array INT_combined.
        Here we perform HDBSCAN clustering on the normalised data from Prepdata.
        Labels and Probaility are also  stored from  and they are stored.
        '''
        
        self.INT_un=INT_un
        self.INT_n=INT_n
        self.i=i
        
        INT_un=np.load('INT_un.npz')
        INT_un=INT_un['arr_0']
        INT_n=np.load('INT_n.npz')
        INT_n=INT_n['arr_0']
        
        
        start=time.time()
        clusterer=hdbscan.HDBSCAN(min_cluster_size=17,
                                  min_samples=16,
                                  cluster_selection_epsilon=0.01,
                                  allow_single_cluster=False,
                                  cluster_selection_method='eom')
        clusterer.fit(INT_n[:,self.i,:])
        INT_n_part=INT_n[:,self.i,:]
        INT_2d=clusterer.labels_
        INT_prob=clusterer.probabilities_
        INT_exemp_part=clusterer.exemplars_
        n_clusters_=len(set(INT_2d)) -(1 if -1 in INT_2d else 0)
        end=time.time()
        print(end-start)
        print(n_clusters_)

        get_ipython().run_line_magic('matplotlib', 'inline')
        plt.figure(figsize=(5,5))


        color_palette = sns.color_palette('Paired',n_clusters_)
        cluster_colors = [color_palette[x] if x >= 0
                          else (0.5, 0.5, 0.5)
                          for x in INT_2d]
        cluster_member_colors = [sns.desaturate(x, p) for x, p in
                                 zip(cluster_colors, clusterer.probabilities_)]
        #plt.scatter(*data.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)
        #plt.title('HDBSCAN for min_cluster_size=30, min_sample=30')
        plt.xlabel('INT0')
        plt.ylabel('INT1')
        plt.title('HDBSCAN on single frequencies')
        plt.scatter(INT_n_part[:,0],
                    INT_n_part[:,1],
                    marker=".",
                    alpha=1,
                    c=cluster_member_colors) #color='b')

        
        
    def Add(self,
            INT_un,
            INT_n,
            INT_combined1):
         '''
        Here we use Cluster exemplar addition method.
        We feed the unnormalised intensity array at INT_un and
        the normalised intensity array INT_n and the normalisation array INT_combined.
        Here we perform HDBSCAN clustering on the normalised data from Prepdata.
        Labels and Probaility are also  stored from  and they are stored. Exemplars 
        are also stored from HDBSCAN. Then the following algorithm is used:
        1) Do clustering and save the exemplars.
        2) ADD the nearest exemplar from the normalized 3 beam data.
        3) Multiply the normalization with subtracted data.
        More details is in Thesis.
        '''
        
            self.INT_un=INT_un
            self.INT_n=INT_n
            self.INT_combined1=INT_combined1

            INT_un=np.load('INT_un.npz')
            INT_un=INT_un['arr_0']

            INT_n=np.load('INT_n.npz')
            INT_n=INT_n['arr_0']

            INT_combined1=np.load('INT_combined1.npz')
            INT_combined1=INT_combined1['arr_0']
 
            
            #entire 16384 channels HDBSCAN
            start=time.time()

            INT_prob1=[]  #probability
            INT_2d=[]     #labels
            INT_exemp=[]  #exemplars
            for i in range(0,INT_n.shape[1]):
                clusterer=hdbscan.HDBSCAN(min_cluster_size=20,
                                          min_samples=None)
                clusterer.fit(INT_n[:,i,:])
                INT_pred=clusterer.labels_
                n_clusters_=len(set(INT_pred)) -(1 if -1 in INT_pred else 0)
                INT_pred=clusterer.labels_
                INT_prob=clusterer.probabilities_

                INT_exemp.append(clusterer.exemplars_)
                INT_2d.append(INT_pred)
                INT_prob1.append(INT_prob)


            INT_prob1=np.array(INT_prob1)
            INT_2d=np.array(INT_2d)
            INT_exemp=np.array(INT_exemp)
            
            savez_compressed('INT_prob1.npz',INT_prob1)
            savez_compressed('INT_2d.npz',INT_2d)
            savez_compressed('INT_exemp.npz',INT_exemp)



            #exemplar subtraction


            INT_n_part=INT_n
            INT_2d_part=INT_2d
            INT_exemp_part=INT_exemp
            INTnew=np.zeros_like(INT_n_part)     #the cleaned norm array
            ARGMIN=[]

            for i in range(0,INT_2d.shape[0]):
                for j in range(0,INT_2d.shape[1]):
                    INT_exemp_combined=np.vstack(INT_exemp_part[i])

                    label=INT_2d_part[i,j]
                    value=INT_n_part[j,i,:]

                    if label!=-1:

                        argmin=np.argmin(np.sum((value-INT_exemp_part[i][label])**2,axis=1))
                        INTnew[j,i,:]=value+(INT_exemp_part[i][label][argmin])

                    else:
                        argmin=np.argmin(np.sum((value-INT_exemp_combined)**2,axis=1))
                        INTnew[j,i,:]=value+INT_exemp_combined[argmin]

                    ARGMIN.append(argmin)    


            end=time.time()
            print(end-start)
            print(INTnew.shape)    
            savez_compressed('INTnew.npz',INTnew)


            #denormalise the array
            INTnewnorm_whole=(INTnew.T)/INT_combined1 #cleaned unnorm array
            savez_compressed('INTnewnorm_whole.npz',(INTnewnorm_whole))
            
            
    def Iautils(self, 
                INTnewnorm_whole, 
                INT_un, 
                Weight, 
                fpga0,
                fpgan,
                dm,
                beam):
        
        '''
         
        After the exemplar subtraction has been done, the subtracted array is denormalized
        using the normalization array we had saved in the beginning. Next on the denormalized array
        from all 3 beams we apply various routines from IAUTILS.
        The regular workflow on IAUTILS is as follows:
        1) Declare various constants and input parameters to be passed to cascade object in
        step 2.
        2) Make a cascade object containing data/ spectrum with cascade.py .
        3) Dedisperse the data in a cascade object.
        4) Subband the data in a cascade object.
        5) Apply various analysis routines to detect pulse, determine SNR, optimise DM,
        generate TIMESERIES on the spectra object. After dedispersion and subbanding a
        spectra object is created within the cascade object from spectra.py script that works in
        tandem with cascade.py
        '''
        
        
         
        #IAUTILS
        self.INT_un= INT_un
        self.INTnewnorm_whole= INTnewnorm_whole
        self.Weight=Weight
        self.fpga0=fpga0
        self.fpgan=fpgan
        self.dm=dm
        self.beam=beam
        
        INT_un=np.load('INT_un.npz')
        INT_un=INT_un['arr_0']
        
        INTnewnorm_whole=np.load('INTnewnorm_whole.npz')
        INTnewnorm_whole=INTnewnorm_whole['arr_0']
        
        Weight=np.load('Weight.npz')
        Weight=Weight['arr_0']
        
        fpga0=np.load('fpga0.npz')
        fpga0=fpga0['arr_0']

        fpgan=np.load('fpgan.npz')
        fpgan=fpgan['arr_0']



        INT_un1st=INT_un[0,:,:]
        INT_un2nd=INT_un[1,:,:]
        INT_un3rd=INT_un[2,:,:]


       
        INTnewnorm_whole1st=INTnewnorm_whole[0,:,:]
        INTnewnorm_whole2nd=INTnewnorm_whole[1,:,:]
        INTnewnorm_whole3rd=INTnewnorm_whole[2,:,:]


       
        Weight1=Weight[0,:,:]
        Weight2=Weight[1,:,:]
        Weight3=Weight[2,:,:]


        fpga01st=fpga0[0]
        fpga02nd=fpga0[1]
        fpga03rd=fpga0[2]


        fpgan1st=fpgan[0]
        fpgan2nd=fpgan[1]
        fpgan3rd=fpgan[2]

        '''
        #making copies of arrays
        import copy 
        INTnewnorm_whole1st=copy.copy(INTnewnorm_whole1st)
        INT_un1st=copy.copy(INT_un1st)
        
        INTnewnorm_whole2nd=copy.copy(INTnewnorm_whole2nd)
        INT_un2nd=copy.copy(INT_un2nd)
        
        INTnewnorm_whole3rd=copy.copy(INTnewnorm_whole3rd)
        INT_un3rd=copy.copy(INT_un3rd)
        '''        
        
        
        #CASCADE OBJECT


        #meanSTD removal
        '''
        INTcascade=INTnewnorm_whole2nd

        INTmeanstd=[]
        for i in range(len(INTcascade)):
            MEAN=np.nanmean(INTcascade[i,:])
            INTMEANREMOVED=INTcascade[i,:]-MEAN
            INTSTD=np.nanstd(INTcascade[i,:])
            INTMEANSTD=INTMEANREMOVED/INTSTD
            INTmeanstd.append(INTMEANSTD)
        INTmeanstd=np.array(INTmeanstd)
       
        '''
        
        
        
        
        
        
  '''
        Parameters

        eventid : int
            CHIME/FRB event ID.
        intensities : list of array_like
            List of 2D NumPy arrays containing radio intensity data for each
            beam.
        weights : list of array_like
            List of NumPy arrays of weights. 
        fbottom : float
            Bottom observing frequency of the receiver bandwidth, in MHz.
        df : float
            Frequency channel width, in MHz.
        tstart : float
            Time at the start of the first sample, in s.
        dt : float or list
            Sampling time, in s. 
        dm : float
            Dispersion measure, in pc cm-3.
      
        beam_nos : list of str
            Beam numbers (or names).
        fpga0s : list of array_like
            A list of FPGA zero-times of the first sample in each chunk.
            Each list element should be an array of fpga0s for each beam.
        fpgaNs : list of array_like
            List of numbers of time samples in each chunk.
            Each list element should be an array of fpgaNs for each beam.
        event_time : datetime
            UTC time stamp of the event.
        fpga_time : int
            FPGA time stamp of the event.
        time_range : float
            Bonsai event time uncertainty (due to coarse-graining).
        dm_range : float
            Bonsai DM uncertainty (due to coarse-graining).
      
        frame0_nanos : list of ndarray
            List of frame0_nanos for each beam.
         
        binnings: list of floats
            List of binning factors of the data for each beam.
        detection_beam_nos : list of str
            Beam numbers (or names) of beams that made detections.
        max_beam_no : str
            Beam number (or names) of beam that had the max S/N.
        use_weights: bool, optional
            Use the weights returned by rpc_client (removes dead channels,
            negative intensities). Defaults to True.
        use_rfi_masks: bool, optional
            Use rfi_masks returned by rpc_client (removes values marked as
            RFI by rf_pipelines). Not applied if using fitburst preprocessing.
      
        '''

        event_time = datetime.datetime.utcnow()
        fpga_time=481426529279
        time_range = 0.1
        dm_range = 0.5
        beam = beam

        
        if beam=='1_UNCLEAN':
            INTEN=INT_un1st
            WEIGHT=Weight1
            FPGA0=fpga01st
            FPGAN=fpgan1st
            
        elif beam=='1_CLEAN':
            INTEN=INTnewnorm_whole1st
            WEIGHT=Weight1
            FPGA0=fpga01st
            FPGAN=fpgan1st
        
        elif beam=='2_UNCLEAN':
            INTEN=INT_un2nd
            WEIGHT=Weight2
            FPGA0=fpga02nd
            FPGAN=fpgan2nd
            
        elif beam=='2_CLEAN':
            INTEN=INTnewnorm_whole2nd
            WEIGHT=Weight2
            FPGA0=fpga02nd
            FPGAN=fpgan2nd
            
        elif beam=='3_UNCLEAN':
            INTEN=INT_un3rd
            WEIGHT=Weight3
            FPGA0=fpga03rd
            FPGAN=fpgan3rd
            
        elif beam=='3_CLEAN':
            INTEN=INTnewnorm_whole3rd
            WEIGHT=Weight3
            FPGA0=fpga03rd
            FPGAN=fpgan3rd 
            
            
            
            
      
        event = cascade.Cascade(intensities=[INTEN], 
                                    weights=[WEIGHT], 
                                    beam_nos=[beam],
                                    fbottom=common_utils.freq_bottom_mhz,
                                    df=common_utils.channel_bandwidth_mhz, 
                                    tstart=0.0,
                                    dt=common_utils.sampling_time_ms/1e3*1, 
                                    fpga0s=[FPGA0],
                                    fpgaNs=[FPGAN], 
                                    dm= dm,
                                    event_time=event_time,
                                    fpga_time=int(fpga_time), 
                                    time_range=float(time_range),
                                    dm_range=float(dm_range),

                                    use_rfi_masks=False,
                                   )



            #DEDISPERSE

        event.dm = dm #558.4408569335938 #558.46   #558.4408569335938


            #SUBBANDING

        event.process_cascade(dm=dm, 
                                  nsub=64, 
                                  downsample_factor=1,
                                  dedisperse=True, 
                                  subband=True,
                                  downsample=True, 
                                  mask=False,
                                  zerodm=False, 
                                  scaleindep=True,
                                  )

            #making timeseries
        TIMESERIES=((event.beams[0].generate_timeseries())[0])
            #TIMESERIES[TIMESERIES<0]=0
        get_ipython().run_line_magic('matplotlib', 'inline')
        plt.plot(TIMESERIES)  #[11000:13000])
        print(np.argmax(TIMESERIES))


        FinalData=event.beams[0].intensity
        savez_compressed('FinalData.npz',FinalData)
        savez_compressed('TIMESERIES.npz',TIMESERIES)


    def Waterfaller(self,
                    FinalData,
                    TIMESERIES,
                    start, 
                    end) :
        '''
        Make a waterfall plot of the dedispersed,  subbanded array of
        data from the spectra object which is created within the cascade object
        '''
        
        self.FinalData=FinalData
        self.start=start
        self.end=end
        self.TIMESERIES=TIMESERIES
        
        FinalData=np.load('FinalData.npz')
        FinalData=FinalData['arr_0']
        
        TIMESERIES=np.load('TIMESERIES.npz')
        TIMESERIES=TIMESERIES['arr_0']
       
        #MANUAL WATERFALLING
        SNR=spectra.smoothed_peak_snr(timeseries=TIMESERIES[self.start:self.end],width=2)
        print('smoothed peaked SNR:',SNR[0])
        get_ipython().run_line_magic('matplotlib', 'inline')
        plt.figure(figsize=(10,10))
        plt.subplot(2,1,1)
        plt.pcolormesh((FinalData[:,self.start:self.end]),
                       cmap='viridis',
                       vmin=-4,
                       vmax=4)
        #plt.pcolormesh((event.beams[0].intensity[:,12200:12300]),cmap='viridis',vmin=-4,vmax=4)
        #plt.subplot(2,1,1)
        #plt.plot(TIMESERIES[12200:12300])

        plt.xlabel('time in ms')
        plt.ylabel('freq subbands')
        plt.colorbar() 
        
        
        plt.figure(figsize=(10,10))
        plt.subplot(2,1,2)    
        plt.xlabel('TIME in ms')
        plt.ylabel('INTENSITY')
        plt.plot(TIMESERIES[self.start:self.end])
        
        
        #FIND BURST SNR
        spectra.find_burst((TIMESERIES[self.start:self.end]), 
                                  width_factor=10, 
                                  min_width=1, 
                                  max_width=10, 
                                  plot=True)
        
        
       
        
       

'''
function='CLUSTERUTILS'
REM=CluCHIME(function)





Pathname1='/DATA/shriharsh/CHIME/multibeam_ML/astro_155769173/intensity/raw/1153/astro*.msgpack'
Pathname2='/DATA/shriharsh/CHIME/multibeam_ML/astro_155769173/intensity/raw/2153/astro*.msgpack'
Pathname3='/DATA/shriharsh/CHIME/multibeam_ML/astro_155769173/intensity/raw/3153/astro*.msgpack'

REM.Prepdata(Pathname1,Pathname2,Pathname3)





INT_un='INT_un.npz'
INT_n='INT_n.npz
INT_combined1='INT_combined1.npz
REM.Sub(INT_un,INT_n,INT_combined1)





INT_un='INT_un.npz'
INT_n='INT_n.npz'
i=2694



REM.Single(INT_un,INT_n,i)





INT_un='INT_un.npz'
INT_n='INT_n.npz
INT_combined1='INT_combined1.npz
REM.Add(INT_un,INT_n,INT_combined1)





INT_un='INT_un.npz'
INTnewnorm_whole='INT_newnorm_whole.npz'
Weight='Weight.npz'
fpga0='fpga0.npz'
fpgan='fpgan.npz'
dm=558.5
beam='3153CLEAN'

REM.Iautils(INT_un,INTnewnorm_whole,Weight,fpga0,fpgan,dm,beam)





FinalData='FinalData.npz'
TIMESERIES='TIMESERIES.npz'
start=12200
end=12400
REM.Waterfaller(FinalData,TIMESERIES,start,end)
'''





